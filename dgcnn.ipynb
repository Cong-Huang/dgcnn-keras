{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Model \n",
    "from keras.optimizers import Nadam\n",
    "import keras.backend as K \n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 膨胀门卷积\n",
    "来源：https://spaces.ac.cn/archives/5409\n",
    "\n",
    "#### Y = Conv1D1(X) ⊗ σ(Conv1D2(X))\n",
    "两个Conv1D形式一样（比如卷积核数、窗口大小都一样），但权值不共享，即参数翻倍，其中一个用sigmoid函数激活，\n",
    "另外一个不加激活函数，然后将它们逐位相乘。\n",
    "\n",
    "因为sigmoid函数的值域是(0,1)，相当于给Conv1D的每个输出都加一个“阀门”来控制流量。\n",
    "这就是GCNN的结构，或可以将这种结构看成一个激活函数，称为GLU（Gated Linear Unit）。\n",
    "\n",
    "除直观意义外，用GCNN的一个好处是几乎不用担心梯度消失问题，因为有一个卷积是不加任意激活函数的，\n",
    "所以对这部分求导是个常数（乘以门），可以说梯度消失的概率非常小。\n",
    "如果输入和输出的维度大小一致，那么我们就把输入也加到里边，即使用残差结构：\n",
    "#### Y = X + Conv1D1(X) ⊗ σ(Conv1D2(X))\n",
    "值得一提的是，使用残差结构，并不只是为了解决梯度消失，而是使得信息能够在多通道传输。\n",
    "可以将上式改写为更形象的等价形式，以便我们更清晰看到信息是如何流动的：\n",
    "#### Y = X ⊗ (1−σ) + Conv1D1(X) ⊗ σ\n",
    "#### σ = σ(Conv1D2(X))\n",
    "上式中我们能更清楚看到信息的流向：以1−σ的概率直接通过，以σ的概率经过变换后才通过。\n",
    "##### 补充推导  Y = X ⊗ (1−σ(Conv1D2(X))) + Conv1D1(X)⊗σ(Conv1D2(X)) = X + (Conv1D1(X)−X)⊗σ(Conv1D2(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dilated_gated_conv1d(seq, mask, dilation_rate=1):\n",
    "    \"\"\"膨胀门卷积（残差式）\n",
    "    \"\"\"\n",
    "    dim = K.int_shape(seq)[-1]    # size\n",
    "    h = Conv1D(dim*2, 3, padding='same', dilation_rate=dilation_rate)(seq)   # (bs, sl, size*2)\n",
    "    def _gate(x):\n",
    "        dropout_rate = 0.1\n",
    "        s, h = x   # (bs, sl, size)  (bs, sl, size*2)\n",
    "        g, h = h[:, :, :dim], h[:, :, dim:]   #  (bs, sl, size)  (bs, sl, size)\n",
    "        g = K.in_train_phase(K.dropout(g, dropout_rate), g)   # 训练中dropout\n",
    "        g = K.sigmoid(g)   # (bs, sl, size)\n",
    "        return g * s + (1 - g) * h   \n",
    "    seq = Lambda(_gate)([seq, h])\n",
    "    seq = Lambda(lambda x: x[0] * x[1])([seq, mask])\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_feature = 20000\n",
    "max_len = 300\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 300) (25000,) (25000, 300) (25000,)\n"
     ]
    }
   ],
   "source": [
    "x_train = pad_sequences(x_train, maxlen=max_len, padding='post', truncating='post')\n",
    "x_test = pad_sequences(x_test, maxlen=max_len, padding='post', truncating='post')\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 300, 1)\n"
     ]
    }
   ],
   "source": [
    "x_in = Input(shape=(max_len,))\n",
    "mask = Lambda(lambda x: K.cast(K.greater(K.expand_dims(x, 2), 0), 'float32'))(x_in)\n",
    "print(K.int_shape(mask))\n",
    "x = Embedding(max_feature+1, 100)(x_in)\n",
    "x = BatchNormalization()(x)\n",
    "x = SpatialDropout1D(0.2)(x)\n",
    "# x = Conv1D(128, 3, activation='relu')(x)\n",
    "x = Dilated_gated_conv1d(x, mask, dilation_rate=1)\n",
    "x = Dilated_gated_conv1d(x, mask, dilation_rate=2)\n",
    "x = Dilated_gated_conv1d(x, mask, dilation_rate=4)\n",
    "x = Dilated_gated_conv1d(x, mask, dilation_rate=1)\n",
    "x = Dilated_gated_conv1d(x, mask, dilation_rate=1)\n",
    "x_avg = GlobalAveragePooling1D()(x)\n",
    "x_max = GlobalMaxPooling1D()(x)\n",
    "x = Concatenate()([x_avg, x_max])\n",
    "x = Dense(100, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x_out = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(x_in, x_out)\n",
    "model.compile(optimizer=Nadam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=2, min_lr=0.5e-6)\n",
    "early_stopper = EarlyStopping(min_delta=0.001, patience=5)\n",
    "model_checkpoint= ModelCheckpoint(\"save_model/model_weights.h5\", \n",
    "                                  monitor=\"val_loss\", \n",
    "                                  save_best_only=True, \n",
    "                                  save_weights_only=True, mode='auto')\n",
    "callbacks_list=[lr_reducer, early_stopper, model_checkpoint] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test), callbacks=callbacks_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
